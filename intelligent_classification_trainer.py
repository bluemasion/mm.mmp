#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½åˆ†ç±»è®­ç»ƒç³»ç»Ÿ
åŸºäºç”¨æˆ·æä¾›çš„è®­ç»ƒæ•°æ®æ”¹è¿›åˆ†ç±»ç®—æ³•
"""

import pandas as pd
import numpy as np
import re
import sqlite3
from typing import Dict, List, Any, Tuple
from datetime import datetime
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntelligentClassificationTrainer:
    """æ™ºèƒ½åˆ†ç±»è®­ç»ƒå™¨"""
    
    def __init__(self, master_data_path: str = 'e4p9.csv', new_data_path: str = 'e4.xlsx'):
        self.master_data_path = master_data_path
        self.new_data_path = new_data_path
        self.master_data = None
        self.new_data = None
        self.tfidf_vectorizer = None
        self.trained_patterns = {}
        
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ”§ åŠ è½½è®­ç»ƒæ•°æ®...")
        
        try:
            # åŠ è½½ä¸»æ•°æ® (e4p9.csv)
            self.master_data = pd.read_csv(self.master_data_path)
            logger.info(f"âœ… ä¸»æ•°æ®åŠ è½½æˆåŠŸ: {self.master_data.shape}")
            
            # åŠ è½½æ–°æ•°æ® (e4.xlsx)
            self.new_data = pd.read_excel(self.new_data_path)
            logger.info(f"âœ… æ–°æ•°æ®åŠ è½½æˆåŠŸ: {self.new_data.shape}")
            
            # æ•°æ®é¢„å¤„ç†
            self._preprocess_data()
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        # ç»Ÿä¸€å­—æ®µåç§°æ˜ å°„
        master_mapping = {
            'spczm': 'id',
            'äº§å“åç§°': 'name',
            'äº§å“è§„æ ¼': 'specification',
            'ç”Ÿäº§å‚å®¶': 'manufacturer',
            'åŒ»ä¿ä»£ç ': 'insurance_code'
        }
        
        new_mapping = {
            'èµ„äº§ä»£ç ': 'id', 
            'èµ„äº§åç§°': 'name',
            'è§„æ ¼å‹å·': 'specification',
            'å“ç‰Œ': 'brand',
            'åŒ»ä¿ç ': 'insurance_code',
            'ç”Ÿäº§å‚å®¶åç§°': 'manufacturer'
        }
        
        # é‡å‘½åå­—æ®µ
        self.master_data = self.master_data.rename(columns=master_mapping)
        self.new_data = self.new_data.rename(columns=new_mapping)
        
        # æ¸…ç†æ•°æ®
        for df in [self.master_data, self.new_data]:
            df['name'] = df['name'].fillna('')
            df['specification'] = df['specification'].fillna('')
            df['manufacturer'] = df['manufacturer'].fillna('')
            
            # åˆå¹¶æ–‡æœ¬ç”¨äºåˆ†æ
            df['combined_text'] = df['name'].astype(str) + ' ' + \
                                df['specification'].astype(str) + ' ' + \
                                df['manufacturer'].astype(str)
        
        logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
    
    def extract_classification_patterns(self):
        """æå–åˆ†ç±»æ¨¡å¼"""
        logger.info("ğŸ§  æå–åˆ†ç±»æ¨¡å¼...")
        
        # åˆ†æäº§å“åç§°ä¸­çš„å…³é”®è¯
        product_keywords = self._extract_product_keywords()
        
        # åˆ†æåˆ¶é€ å•†æ¨¡å¼
        manufacturer_patterns = self._extract_manufacturer_patterns()
        
        # åˆ†æè§„æ ¼æ¨¡å¼
        specification_patterns = self._extract_specification_patterns()
        
        # æ„å»ºåˆ†ç±»è§„åˆ™
        classification_rules = self._build_classification_rules(
            product_keywords, manufacturer_patterns, specification_patterns
        )
        
        return classification_rules
    
    def _extract_product_keywords(self) -> Dict[str, List[str]]:
        """æå–äº§å“å…³é”®è¯"""
        logger.info("ğŸ“ åˆ†æäº§å“å…³é”®è¯...")
        
        # åˆå¹¶æ‰€æœ‰äº§å“åç§°
        all_names = list(self.master_data['name']) + list(self.new_data['name'])
        
        # ä½¿ç”¨jiebaåˆ†è¯
        all_words = []
        for name in all_names:
            if pd.notna(name):
                words = jieba.lcut(str(name))
                all_words.extend([w for w in words if len(w) > 1])
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in all_words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åºå¹¶åˆ†ç±»
        high_freq_words = {k: v for k, v in word_freq.items() if v >= 5}
        
        # åˆ†ç±»å…³é”®è¯
        categories = {
            'åŒ»ç–—å™¨æ¢°': ['å™¨æ¢°', 'è®¾å¤‡', 'ä»ªå™¨', 'ç³»ç»Ÿ', 'æœºå™¨', 'æ²»ç–—', 'è¯Šæ–­', 'ç›‘æŠ¤'],
            'ç”µå­è®¾å¤‡': ['ç”µå­', 'ç”µè·¯', 'èŠ¯ç‰‡', 'ä¼ æ„Ÿå™¨', 'æ§åˆ¶å™¨', 'æ˜¾ç¤ºå™¨'],
            'æœºæ¢°é›¶ä»¶': ['é›¶ä»¶', 'é…ä»¶', 'è½´æ‰¿', 'é½¿è½®', 'èºä¸', 'èºæ “'],
            'åŒ–å·¥ææ–™': ['åŒ–å­¦', 'æº¶æ¶²', 'è¯•å‰‚', 'ææ–™', 'æ¶‚æ–™', 'èƒ¶æ°´']
        }
        
        return {'word_frequency': high_freq_words, 'category_keywords': categories}
    
    def _extract_manufacturer_patterns(self) -> Dict[str, str]:
        """åˆ†æåˆ¶é€ å•†æ¨¡å¼"""
        logger.info("ğŸ¢ åˆ†æåˆ¶é€ å•†æ¨¡å¼...")
        
        manufacturer_category = {}
        
        # åˆå¹¶åˆ¶é€ å•†æ•°æ®
        all_manufacturers = list(self.master_data['manufacturer'].dropna()) + \
                          list(self.new_data['manufacturer'].dropna())
        
        # åˆ¶é€ å•†åˆ†ç±»è§„åˆ™
        medical_keywords = ['åŒ»ç–—', 'å™¨æ¢°', 'ç”Ÿç‰©', 'è¯ä¸š', 'åŒ»è¯', 'å¥åº·', 'å¼ºç”Ÿ', 'Johnson', 'Depuy']
        electronic_keywords = ['ç”µå­', 'ç§‘æŠ€', 'æŠ€æœ¯', 'è½¯ä»¶', 'æ•°ç ', 'é€šè®¯']
        mechanical_keywords = ['æœºæ¢°', 'å·¥ç¨‹', 'åˆ¶é€ ', 'é‡å·¥', 'ç²¾å¯†', 'è‡ªåŠ¨åŒ–']
        
        for manufacturer in set(all_manufacturers):
            manufacturer_str = str(manufacturer).lower()
            
            if any(keyword in manufacturer_str for keyword in medical_keywords):
                manufacturer_category[manufacturer] = 'åŒ»ç–—å™¨æ¢°'
            elif any(keyword in manufacturer_str for keyword in electronic_keywords):
                manufacturer_category[manufacturer] = 'ç”µå­è®¾å¤‡'
            elif any(keyword in manufacturer_str for keyword in mechanical_keywords):
                manufacturer_category[manufacturer] = 'æœºæ¢°é›¶ä»¶'
            else:
                manufacturer_category[manufacturer] = 'é€šç”¨'
        
        return manufacturer_category
    
    def _extract_specification_patterns(self) -> Dict[str, List[str]]:
        """åˆ†æè§„æ ¼æ¨¡å¼"""
        logger.info("ğŸ“ åˆ†æè§„æ ¼æ¨¡å¼...")
        
        # åˆå¹¶æ‰€æœ‰è§„æ ¼æ•°æ®
        all_specs = list(self.master_data['specification'].dropna()) + \
                   list(self.new_data['specification'].dropna())
        
        # è§„æ ¼æ¨¡å¼
        patterns = {
            'å°ºå¯¸è§„æ ¼': [],
            'ç”µæ°”è§„æ ¼': [],
            'æè´¨è§„æ ¼': [],
            'åŒ»ç–—è§„æ ¼': []
        }
        
        size_pattern = r'(\d+(?:\.\d+)?)\s*[Ã—xX*]\s*(\d+(?:\.\d+)?)'
        voltage_pattern = r'(\d+(?:\.\d+)?)\s*[VvKk]'
        material_pattern = r'(ä¸é”ˆé’¢|é“åˆé‡‘|å¡‘æ–™|æ©¡èƒ¶|ç¡…èƒ¶)'
        medical_pattern = r'(ä¸€æ¬¡æ€§|æ— èŒ|æ¶ˆæ¯’|ç­èŒ)'
        
        for spec in all_specs:
            spec_str = str(spec)
            
            if re.search(size_pattern, spec_str):
                patterns['å°ºå¯¸è§„æ ¼'].append(spec_str)
            if re.search(voltage_pattern, spec_str):
                patterns['ç”µæ°”è§„æ ¼'].append(spec_str)
            if re.search(material_pattern, spec_str):
                patterns['æè´¨è§„æ ¼'].append(spec_str)
            if re.search(medical_pattern, spec_str):
                patterns['åŒ»ç–—è§„æ ¼'].append(spec_str)
        
        return patterns
    
    def _build_classification_rules(self, keywords, manufacturers, specifications):
        """æ„å»ºåˆ†ç±»è§„åˆ™"""
        logger.info("âš™ï¸ æ„å»ºåˆ†ç±»è§„åˆ™...")
        
        rules = {
            'keyword_rules': {},
            'manufacturer_rules': manufacturers,
            'specification_rules': {},
            'combined_rules': {}
        }
        
        # åŸºäºå…³é”®è¯çš„è§„åˆ™
        for category, words in keywords['category_keywords'].items():
            rules['keyword_rules'][category] = {
                'keywords': words,
                'confidence_base': 0.7
            }
        
        # åŸºäºè§„æ ¼çš„è§„åˆ™
        rules['specification_rules'] = {
            'åŒ»ç–—å™¨æ¢°': ['ç”µæ', 'å¯¼ç®¡', 'æ¤å…¥ç‰©', 'ä¸€æ¬¡æ€§'],
            'ç”µå­è®¾å¤‡': ['V', 'W', 'Hz', 'A', 'Î©'],
            'æœºæ¢°é›¶ä»¶': ['mm', 'cm', 'M3', 'M4', 'ä¸é”ˆé’¢']
        }
        
        return rules
    
    def train_tfidf_model(self):
        """è®­ç»ƒTF-IDFæ¨¡å‹"""
        logger.info("ğŸ¤– è®­ç»ƒTF-IDFæ¨¡å‹...")
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬æ•°æ®
        all_texts = list(self.master_data['combined_text']) + \
                   list(self.new_data['combined_text'])
        
        # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,  # ä¸­æ–‡ä¸ä½¿ç”¨è‹±æ–‡åœç”¨è¯
            ngram_range=(1, 2)
        )
        
        # è®­ç»ƒæ¨¡å‹
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)
        
        logger.info(f"âœ… TF-IDFæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œç‰¹å¾ç»´åº¦: {tfidf_matrix.shape}")
        
        return tfidf_matrix
    
    def generate_enhanced_keywords(self):
        """ç”Ÿæˆå¢å¼ºçš„å…³é”®è¯æ˜ å°„"""
        logger.info("ğŸš€ ç”Ÿæˆå¢å¼ºå…³é”®è¯æ˜ å°„...")
        
        # åˆ†æå®é™…æ•°æ®ä¸­çš„é«˜é¢‘è¯
        all_product_names = []
        for df in [self.master_data, self.new_data]:
            all_product_names.extend(df['name'].dropna().tolist())
        
        # ä½¿ç”¨jiebaåˆ†è¯å¹¶ç»Ÿè®¡
        word_counts = {}
        for name in all_product_names:
            words = jieba.lcut(str(name))
            for word in words:
                if len(word) > 1:  # è¿‡æ»¤å•å­—ç¬¦
                    word_counts[word] = word_counts.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        
        # ç”Ÿæˆå¢å¼ºçš„åˆ†ç±»å…³é”®è¯
        enhanced_keywords = {
            'åŒ»ç–—å™¨æ¢°': [],
            'æ²»ç–—è®¾å¤‡': [],
            'è¯Šæ–­è®¾å¤‡': [],
            'æ‰‹æœ¯å™¨æ¢°': [],
            'ç›‘æŠ¤è®¾å¤‡': []
        }
        
        # åŒ»ç–—ç›¸å…³é«˜é¢‘è¯
        medical_words = [word for word, count in sorted_words[:100] 
                        if any(med_key in word for med_key in 
                              ['åŒ»', 'ç–—', 'æ¢°', 'è®¾å¤‡', 'ä»ªå™¨', 'ç³»ç»Ÿ', 'æ²»ç–—', 'æ‰‹æœ¯', 'è¯Šæ–­'])]
        
        enhanced_keywords['åŒ»ç–—å™¨æ¢°'].extend(medical_words[:20])
        enhanced_keywords['æ²»ç–—è®¾å¤‡'].extend([w for w in medical_words if 'æ²»ç–—' in w or 'å°„é¢‘' in w or 'ç­‰ç¦»å­' in w])
        enhanced_keywords['æ‰‹æœ¯å™¨æ¢°'].extend([w for w in medical_words if 'æ‰‹æœ¯' in w or 'ç”µæ' in w or 'å¯¼ç®¡' in w])
        
        return enhanced_keywords
    
    def save_training_results(self, output_path: str = 'training_results.json'):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        logger.info(f"ğŸ’¾ ä¿å­˜è®­ç»ƒç»“æœåˆ° {output_path}...")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'data_summary': {
                'master_data_count': len(self.master_data),
                'new_data_count': len(self.new_data),
                'total_samples': len(self.master_data) + len(self.new_data)
            },
            'classification_patterns': self.trained_patterns,
            'enhanced_keywords': self.generate_enhanced_keywords()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info("âœ… è®­ç»ƒç»“æœä¿å­˜æˆåŠŸ")
    
    def update_database_categories(self):
        """æ›´æ–°æ•°æ®åº“åˆ†ç±»æ•°æ®"""
        logger.info("ğŸ—„ï¸ æ›´æ–°æ•°æ®åº“åˆ†ç±»æ•°æ®...")
        
        conn = sqlite3.connect('master_data.db')
        cursor = conn.cursor()
        
        # åŸºäºè®­ç»ƒæ•°æ®æ·»åŠ æ–°çš„åˆ†ç±»
        new_categories = [
            ('CAT004', 'è¯Šç–—è®¾å¤‡', 'CAT003', 2, 'ä¸“ä¸šè¯Šç–—è®¾å¤‡', '{"keywords": ["è¯Šç–—", "æ£€æŸ¥", "æ‰«æ"]}'),
            ('CAT005', 'æ¤å…¥å™¨æ¢°', 'CAT003', 2, 'æ¤å…¥å¼åŒ»ç–—å™¨æ¢°', '{"keywords": ["æ¤å…¥", "åŸ‹ç½®", "å†…ç½®"]}'),
            ('CAT006', 'æ€¥æ•‘è®¾å¤‡', 'CAT003', 2, 'æ€¥æ•‘åŒ»ç–—è®¾å¤‡', '{"keywords": ["æ€¥æ•‘", "æŠ¢æ•‘", "åº”æ€¥"]}')
        ]
        
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        iso_time = datetime.now().isoformat()
        
        for category in new_categories:
            try:
                cursor.execute('''
                INSERT OR IGNORE INTO material_categories 
                (category_id, category_name, parent_id, level, description, features, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (*category, current_time, iso_time))
                logger.info(f"  æ·»åŠ åˆ†ç±»: {category[1]}")
            except Exception as e:
                logger.warning(f"  åˆ†ç±»æ·»åŠ å¤±è´¥ {category[1]}: {e}")
        
        conn.commit()
        conn.close()
        logger.info("âœ… æ•°æ®åº“åˆ†ç±»æ•°æ®æ›´æ–°å®Œæˆ")
    
    def run_training_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨æ™ºèƒ½åˆ†ç±»è®­ç»ƒæµç¨‹...")
        
        try:
            # 1. åŠ è½½æ•°æ®
            self.load_training_data()
            
            # 2. æå–åˆ†ç±»æ¨¡å¼
            self.trained_patterns = self.extract_classification_patterns()
            
            # 3. è®­ç»ƒTF-IDFæ¨¡å‹
            self.train_tfidf_model()
            
            # 4. æ›´æ–°æ•°æ®åº“
            self.update_database_categories()
            
            # 5. ä¿å­˜ç»“æœ
            self.save_training_results()
            
            logger.info("ğŸ‰ æ™ºèƒ½åˆ†ç±»è®­ç»ƒå®Œæˆï¼")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("    æ™ºèƒ½åˆ†ç±»è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    
    trainer = IntelligentClassificationTrainer()
    
    if trainer.run_training_pipeline():
        print("\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("\nğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
        print(f"   ä¸»æ•°æ®æ ·æœ¬: {len(trainer.master_data)} æ¡")
        print(f"   æ–°æ•°æ®æ ·æœ¬: {len(trainer.new_data)} æ¡") 
        print(f"   æ€»è®­ç»ƒæ ·æœ¬: {len(trainer.master_data) + len(trainer.new_data)} æ¡")
        
        print("\nğŸ¯ å»ºè®®ä¸‹ä¸€æ­¥:")
        print("   1. é‡å¯MMPåº”ç”¨ä»¥åŠ è½½æ–°çš„åˆ†ç±»æ•°æ®")
        print("   2. æµ‹è¯•æ”¹è¿›åçš„æ™ºèƒ½åˆ†ç±»æ•ˆæœ") 
        print("   3. æŸ¥çœ‹ training_results.json äº†è§£è®­ç»ƒè¯¦æƒ…")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—")

if __name__ == "__main__":
    main()